Deep Learning in Action
========================================================
author: 
date: 
autosize: true

First Slide
========================================================

- examples
- neural networks
- convnet.js
- examples again, with a bit of theory

- short intro to other frameworks


Self-driving cars
========================================================


Machine Translation
========================================================

https://arxiv.org/pdf/1609.08144.pdf


Medical Diagnosis
========================================================

![optional caption text](skincancer.png)

http://www.nature.com/nature/journal/vaop/ncurrent/full/nature21056.html



Stock market prediction
========================================================

![optional caption text](stock.png)

Yoshihara et al. "Leveraging temporal properties of news
events for stock market prediction." 2015.


Neural networks
========================================================
pic tbd



Deep neural networks
========================================================
A deep representation is a composition of many functions

$x \xrightarrow{w_1} h_1 \xrightarrow{w_2} h_2 \xrightarrow{w_3} ... \xrightarrow{w_n} h_n \xrightarrow{w_{n+1}} y$


Backpropagation
========================================================
![optional caption text](backprop.png)

http://cs231n.github.io/optimization-2/

// If we anthropomorphize the circuit as wanting to output a higher value (which can help with intuition), then we can think of the circuit as “wanting” the output of the add gate to be lower (due to negative sign), and with a force of 4. To continue the recurrence and to chain the gradient, the add gate takes that gradient and multiplies it to all of the local gradients for its inputs (making the gradient on both x and y 1 * -4 = -4). 


Hard is easy, easy is hard
========================================================

![optional caption text](moravec.png)

MIT Course 6.S094:
Deep Learning for Self-Driving Cars


Convnetjs demo
========================================================

1. shallow network, simple data

```
layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth:2});
layer_defs.push({type:'softmax', num_classes:2});

trainer = new convnetjs.SGDTrainer(net, {learning_rate:0.01, momentum:0.1, batch_size:10, l2_decay:0.001});
```

2. deep network, simple data, 1 hidden layer with tanh activation

```
layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth:2});
layer_defs.push({type:'fc', num_neurons:2, activation: 'tanh'});
layer_defs.push({type:'softmax', num_classes:2});

trainer = new convnetjs.SGDTrainer(net, {learning_rate:0.01, momentum:0.1, batch_size:10, l2_decay:0.001});
```

3. deep network, simple data, change learning rate (0.001)

```
layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth:2});
layer_defs.push({type:'fc', num_neurons:2, activation: 'tanh'});
layer_defs.push({type:'softmax', num_classes:2});

trainer = new convnetjs.SGDTrainer(net, {learning_rate:0.001, momentum:0.1, batch_size:10, l2_decay:0.001});
```

4. deep network, simple data, change learning rate (0.5)

```
layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth:2});
layer_defs.push({type:'fc', num_neurons:2, activation: 'tanh'});
layer_defs.push({type:'softmax', num_classes:2});

trainer = new convnetjs.SGDTrainer(net, {learning_rate:0.5, momentum:0.1, batch_size:10, l2_decay:0.001});
```

5. deep network, simple data, change activation function to relu

```
layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth:2});
layer_defs.push({type:'fc', num_neurons:2, activation: 'relu'});
layer_defs.push({type:'softmax', num_classes:2});

trainer = new convnetjs.SGDTrainer(net, {learning_rate:0.01, momentum:0.1, batch_size:10, l2_decay:0.001});
```

6. deep network, circle data, still 1 hidden layer with relu or tanh activation

```
layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth:2});
layer_defs.push({type:'fc', num_neurons:2, activation: 'relu'});
layer_defs.push({type:'softmax', num_classes:2});

trainer = new convnetjs.SGDTrainer(net, {learning_rate:0.01, momentum:0.1, batch_size:10, l2_decay:0.001});
```

7. deep network, circle data, change number of neurons in hidden unit to 3

```
layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth:2});
layer_defs.push({type:'fc', num_neurons:3, activation: 'tanh'});
layer_defs.push({type:'softmax', num_classes:2});

trainer = new convnetjs.SGDTrainer(net, {learning_rate:0.01, momentum:0.1, batch_size:10, l2_decay:0.001});
```

8. deep network, spiral data, still with tanh activation and 3 neurons

```
layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth:2});
layer_defs.push({type:'fc', num_neurons:3, activation: 'tanh'});
layer_defs.push({type:'softmax', num_classes:2});

trainer = new convnetjs.SGDTrainer(net, {learning_rate:0.01, momentum:0.1, batch_size:10, l2_decay:0.001});
```

9. deep network, spiral data, add another hidden layer with 3 neurons

```
layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth:2});
layer_defs.push({type:'fc', num_neurons:3, activation: 'tanh'});
layer_defs.push({type:'fc', num_neurons:3, activation: 'tanh'});
layer_defs.push({type:'softmax', num_classes:2});

trainer = new convnetjs.SGDTrainer(net, {learning_rate:0.01, momentum:0.1, batch_size:10, l2_decay:0.001});
```

10. deep network, spiral data, more neurons in hidden layers




Representation learning
========================================================
pics from pres



Why computer vision is hard
========================================================

![optional caption text](deformable_cat.png)


http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf


Tasks in computer vision
========================================================

![optional caption text](class_loc_dec_seg.png)

Source: CS231n (Stanford) lecture slides


Classification
========================================================

![optional caption text](classification.png)

A. Krizhevsky, I. Sutskever and G. Hinton, “ImageNet Classification with Deep Convolutional Neural Networks”, in Advances in Neural Information Processing Systems, 2012.

// output probability for each class

Localization
========================================================

![optional caption text](localization.png)

[1] Erhan D., Szegedy C., Toshev, A., and Anguelov, D., "Scalable Object Detection using Deep Neural Networks", The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 2147-2154.

// slide window over image
// or predict bounding box coordinates (regression)


Object Detection
========================================================

![optional caption text](detection.png)

http://image-net.org/challenges/LSVRC/2014/

// = classifying and localizing multiple objects in an image 
// aka image recognition
// binary mask regression for each object type combined with predicting the bounding box coordinates


Segmentation
========================================================

![optional caption text](segmentation.png)

https://arxiv.org/pdf/1411.4038.pdf

//the network needs to predict a class value for each input pixel.


Semantic vs. Instance Segmentation
========================================================

![optional caption text](segmentation.png)

https://arxiv.org/pdf/1411.4038.pdf

@inproceedings{SilbermanCoverage:ECCV14,
  author    = {Nathan Silberman, David Sontag and Rob Fergus},
  title     = {Instance Segmentation of Indoor Scenes using a Coverage Loss},
  booktitle = {ECCV},
  year      = {2014}
}


Convnet visualization
========================================================

https://arxiv.org/pdf/1311.2901.pdf


RNN
========================================================

colah

LSTM "conveyor belt"


Word2Vec
========================================================




Linking images and language: image caption generation
========================================================

![optional caption text](image_captions.png)
![optional caption text](image_captions2.png)

http://cs.stanford.edu/people/karpathy/cvpr2015.pdf


Linking video and sound: adding audio to silent film
========================================================

![optional caption text](sound_generation.png)

https://arxiv.org/pdf/1512.08512.pdf











Reinforcement learning: Welcome to the humans
========================================================

![optional caption text](typesofml.png) 

Source: deep driving

![optional caption text](dopamine.png) 

Reinforcement learning: Welcome to the humans
========================================================

> David Silver, Google Deep Mind: _Reinforcement Learning: AI = RL_

> Takeaway from Supervised Learning:
Neural networks are great at memorization and not (yet)
great at reasoning.
Hope for Reinforcement Learning:
Brute-force propagation of outcomes to knowledge about
states and actions. This is a kind of brute-force “reasoning”.
MIT DD


Delayed reward
========================================================

If I get a reward many many actions later, how do I find out what I'm getting the reward _for_?


Reinforcement learning
========================================================

![optional caption text](robot.png)

MIT



Deep reinforcement learning
========================================================

![optional caption text](dqn.png) 

http://www.nature.com/articles/nature16961.epdf?referrer_access_token=S7uXxNIroKd-0ITVLIW9mdRgN0jAjWel9jnR3ZoTv0OivKk3lXs6SxMz535byYwHnl5-dYSTNp9HCujoL8AwwR39NrI-N0UvQYqpO-G6W-1I6_OXAuVukQ08lbvopRKY2yVJlWWUJvj6gL5qyO8kI3FwsIuw4iSKC-s4RoTnZdVG8WevGFeuMdJ2Zl9cZF7yixAslaF4yKEx3rom3ZszmZBsyuq-9RAnx1XZac4keCI%3D&tracking_referrer=www.nature.com


Deep reinforcement learning
========================================================

![optional caption text](dqn2.png) 

http://icml.cc/2016/tutorials/AlphaGo-tutorial-slides.pdf




