```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE)
#opts_chunk$set(out.width='750px', dpi=200)
```
<style>
.reveal table td{
    border: 0;
}
.reveal table {
    border: 0;
}
.reveal src {
    font-size: 0.6em;
}
.small-code pre code {
  font-size: 1em;
}
.midcenter {
    position: fixed;
    top: 50%;
    left: 50%;
}
.footer {
    position: fixed; 
    top: 90%;
    text-align:right; 
    width:90%;
    margin-top:-150px;
}
.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
</style>


Deep Learning in Action  
========================================================
autosize: true
width: 1440
incremental:true 

Action!
========================================================

<table>
<tr>
<td><img src='self_driving_cars.png' width='300px' /></td><td><img src='translate.png' width='300px'/></td><td><img src='skincancer.png' width='300px'/></td>
</tr>
<tr>
<td><img src='go.png' width='300px' /></td><td><img src='stock.png' width='300px'/></td><td><img src='weather.png' width='300px'/></td>
</tr>
<tr><td class='src'>Sources: [1],[3],[4],[5],[6]</td></tr>
</table>


So what is a neural network?
========================================================

Biological neuron and artificial neuron

<table>
<tr>
<td><img src='neuron1.png' width='400px' /></td><td><img src='neuron2.png' width='400px'/></td>
</tr>
<tr><td class='src'>Source: [10]</td></tr>
</table>


Prototype of a neuron: the perceptron (Rosenblatt, 1958)
========================================================

<table>
<tr>
<td><img src='perceptron.png' width='800px' /></td>
</tr>
<tr><td class='src'>Source: [10]</td></tr>
</table>


Deep neural networks: introducing hidden layers
========================================================

<table>
<tr>
<td><img src='deep_nn.png' width='800px' /></td>
</tr>
<tr><td class='src'>Source: [10]</td></tr>
</table>


Deep neural networks as function composition
========================================================

&nbsp;

A deep representation is a composition of many functions

&nbsp;

$x \xrightarrow{w_1} h_1 \xrightarrow{w_2} h_2 \xrightarrow{w_3} ... \xrightarrow{w_n} h_n \xrightarrow{w_{n+1}} y$



Why go deep? A bit of background
========================================================

&nbsp;

Easy? Difficult?

- walk
- talk
- play chess
- solve matrix computations


Easy for us - difficult for computers
========================================================

&nbsp;

- controlled movement 
- speech recognition
- speech generation
- object recognition and object localization


Representation matters
========================================================

<table>
<tr>
<td><img src='coords.png' width='800px' /></td>
</tr>
<tr><td class='src'>Source: [12]</td></tr>
</table>



Just feed the network the right features?
========================================================

&nbsp;

What are the correct pixel values for a "bike" feature?

- race bike, mountain bike, e-bike?
- pixels in the shadow may be much darker
- what if bike is mostly obscured by rider standing in front?



Let the network pick the features
========================================================

... a layer at a time

<table>
<tr>
<td><img src='features.png' width='600px' /></td>
</tr>
<tr><td class='src'>Source: [12]</td></tr>
</table>



So how does a network learn?
========================================================

&nbsp;

Just a sec - let's meet a real neural network first!

Play around in the browser:

- <a href='http://cs.stanford.edu/people/karpathy/convnetjs/index.html'>ConvNetJS</a>
- <a href='http://cs.stanford.edu/people/karpathy/convnetjs/index.html'>TensorFlow playground</a>


So how DOES a neural network learn?
========================================================

&nbsp;

We need:

- a way to quantify our current (e.g., classification) error
- a way to reduce error on subsequent iterations
- a way to propagate our improvement logic from the output layer all back through the network!


Quantifying error: Loss functions
========================================================

&nbsp;

The _loss_ (or _cost_) function indicates the cost incurred from false prediction / misclassification

Probably the best-known loss function in machine learning is __mean squared error__: 

  $\frac{1}{n} \sum_n{(\hat{y} - y)^2}$
  
Most of the time, in deep learning we use __cross entropy__:

  $- \sum_j{t_j log(y_j)}$
  
This is the negative log probability of the right answer.
  



Learning from errors: Gradient Descent
========================================================


<table>
<tr>
<td><img src='convex.png' width='800px' /></td>
</tr>
<tr><td class='src'>Source: [12]</td></tr>
</table>


Propagate back errors ... well: Backpropagation!
========================================================
incremental:false

&nbsp;

- basically, just the chain rule: $\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}$
- chained over several layers:

<table>
<tr>
<td><img src='backprop2.png' width='600px' /></td>
</tr>
<tr><td class='src'>Source: [14]</td></tr>
</table>


 
Forward pass and backward pass: Intuition
========================================================
incremental:false

- imagine output of $f = (x + y) * z = -12$ "wants" to get bigger
- this could happen by $q$ getting smaller -> $q$ receives negative gradient $\frac{df}{dq} = -4$
- $q$ just passes on this gradient to $x$ and $y$, as  $\frac{dq}{dx} = 1$ and  $\frac{dq}{dy} = 1$
- alternatively, it could happen by $z$ getting bigger -> $z$ receives positive gradient $\frac{df}{dz} = 3$

<table>
<tr>
<td><img src='backprop.png' width='800px' /></td>
</tr>
<tr><td class='src'>Source: [13]</td></tr>
</table>



Applications by example
========================================================

&nbsp;

- CNNs (Convolutional Neural Networks) in computer vision
- RNNs (Recurrent Neural Networks) for Natural Language Processing
- Deep Reinforcement Learning for TBD


Easy vs. hard, revisited
========================================================
incremental:false

&nbsp;

# VISION


Why computer vision is hard
========================================================

<table>
<tr>
<td><img src='deformable_cat.png' width='800px' /></td>
</tr>
<tr><td class='src'>Source: [15]</td></tr>
</table>


Tasks in computer vision
========================================================

<table>
<tr>
<td><img src='class_loc_dec_seg.png' width='1000px' /></td>
</tr>
<tr><td class='src'>Source: [13]</td></tr>
</table>


Classification
========================================================
incremental:false

&nbsp;

In classification, the required output is a probability for each class.

<table>
<tr>
<td><img src='classification.png' width='1000px' /></td>
</tr>
<tr><td class='src'>Source: [13]</td></tr>
</table>



Localization
========================================================
incremental:false

&nbsp;

In localization, the network needs to identify the position of an object in an image.

<table>
<tr>
<td><img src='localization.png' width='600px' /></td>
</tr>
<tr><td class='src'>Source: [17]</td></tr>
</table>



Object Detection
========================================================
incremental:false

&nbsp;

In object detection (a.k.a. image recognition), the network has to classify and localize multiple objects in an image. 

<table>
<tr>
<td><img src='detection.png' width='600px' /></td>
</tr>
<tr><td class='src'>Source: [18]</td></tr>
</table>


Segmentation
========================================================
incremental:false

&nbsp;

In segmentation, the network needs to predict a class value for __each input pixel__.

<table>
<tr>
<td><img src='segmentation.png' width='400px' /></td>
</tr>
<tr><td class='src'>Source: [19]</td></tr>
</table>


Semantic vs. Instance Segmentation
========================================================
incremental:false

&nbsp;

Semantic segmentation segments by __class__, instance segmentation by __class instance__.

<table>
<tr>
<td><img src='segmentation2.png' width='1000px' /></td>
</tr>
<tr><td class='src'>Source: [20]</td></tr>
</table>



Convnet visualization
========================================================

https://arxiv.org/pdf/1311.2901.pdf


RNN
========================================================

colah

LSTM "conveyor belt"


Word2Vec
========================================================




Linking images and language: image caption generation
========================================================

![optional caption text](image_captions.png)
![optional caption text](image_captions2.png)

http://cs.stanford.edu/people/karpathy/cvpr2015.pdf


Linking video and sound: adding audio to silent film
========================================================

![optional caption text](sound_generation.png)

https://arxiv.org/pdf/1512.08512.pdf











Reinforcement learning: Welcome to the humans
========================================================

![optional caption text](typesofml.png) 

Source: deep driving

![optional caption text](dopamine.png) 

Reinforcement learning: Welcome to the humans
========================================================

> David Silver, Google Deep Mind: _Reinforcement Learning: AI = RL_

> Takeaway from Supervised Learning:
Neural networks are great at memorization and not (yet)
great at reasoning.
Hope for Reinforcement Learning:
Brute-force propagation of outcomes to knowledge about
states and actions. This is a kind of brute-force “reasoning”.
MIT DD


Delayed reward
========================================================

If I get a reward many many actions later, how do I find out what I'm getting the reward _for_?


Reinforcement learning
========================================================

![optional caption text](robot.png)

MIT



Deep reinforcement learning
========================================================

![optional caption text](dqn.png) 

http://www.nature.com/articles/nature16961.epdf?referrer_access_token=S7uXxNIroKd-0ITVLIW9mdRgN0jAjWel9jnR3ZoTv0OivKk3lXs6SxMz535byYwHnl5-dYSTNp9HCujoL8AwwR39NrI-N0UvQYqpO-G6W-1I6_OXAuVukQ08lbvopRKY2yVJlWWUJvj6gL5qyO8kI3FwsIuw4iSKC-s4RoTnZdVG8WevGFeuMdJ2Zl9cZF7yixAslaF4yKEx3rom3ZszmZBsyuq-9RAnx1XZac4keCI%3D&tracking_referrer=www.nature.com


Deep reinforcement learning
========================================================

![optional caption text](dqn2.png) 

http://icml.cc/2016/tutorials/AlphaGo-tutorial-slides.pdf



Sources (1)
========================================================
incremental:false

&nbsp;

[1] <a href='http://selfdrivingcars.mit.edu/'>MIT 6.S094: Deep Learning for Self-Driving Cars Lecture Slides</a>

[3] <a href='http://www.nature.com/nature/journal/v542/n7639/full/nature21056.html'>Esteva et al., Dermatologist-level classification of skin cancer with deep neural networks</a>

[4] <a href='https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol'>Wikipedia, AlphaGo versus Lee Sedol</a>

[5] <a href='www.sciedupress.com/journal/index.php/air/article/download/7720/5022'>Yoshihara et al., Leveraging temporal properties of news events for stock market prediction.</a>

[6] <a href='http://www.theweathercompany.com/newsroom/2017/01/23/seasonal-outlook-weather-company-predicts-unusually-mild-weather-early-february'</a> The Weather Company, Seasonal forecast</a>

[7] <a href= 'http://www.dddmag.com/news/2017/02/neural-network-learns-select-potential-anticancer-drugs'>Neural Network Learns to Select Potential Anticancer Drugs</a>

[8] <a href='https://research.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html'>Google Research Blog: AlphaGo: Mastering the ancient game of Go with Machine Learning</a>

[9] <a href='http://www.theweathercompany.com/DeepThunder'>The Weather Company Launches Deep Thunder - the World’s Most Advanced Hyper-Local Weather Forecasting Model for Businesses</a>


Sources (2)
========================================================
incremental:false

&nbsp;

[10] <a href='https://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html'>Christos Stergiou and Dimitrios Siganos, Artificial neurons</a>

[11] <a href='https://uwaterloo.ca/data-science/sites/ca.data-science/files/uploads/files/lecture_1_0.pdf'>University of Waterloo Deep Learning Slides</a>

[12] <a href="http://www.deeplearningbook.org/"> Goodfellow et al., Deep Learning</a>

[13] <a href='http://cs231n.github.io/'>CS231n: Convolutional Neural Networks for Visual Recognition, Stanford</a>

[14] <a href='https://colah.github.io/posts/2015-08-Backprop/'>Chris Olah, Calculus on Computational Graphs: Backpropagation</a>

[15] <a href='http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf'>Parkhi, Vedaldi, Zisserman and Jawahar, Cats and Dogs.</a>

[16] <a href='https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf'>A. Krizhevsky, I. Sutskever and G. Hinton, ImageNet Classification with Deep Convolutional Neural Networks</a>


Sources (3)
========================================================
incremental:false

&nbsp;

[17] <a href='http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf'>Erhan, Szegedy, Toshev, and Anguelov, Scalable Object Detection using Deep Neural Networks</a>

[18] <a href='http://image-net.org/challenges/LSVRC/2014/'>ImageNet Large Scale Visual Recognition Challenge 2014 (ILSVRC2014)</a>

[19] <a href='https://arxiv.org/pdf/1411.4038.pdf'>Long, Shelhamer, and Darrell, Fully Convolutional Networks for Semantic Segmentation.</a>

[20] <a href='http://cs.nyu.edu/~dsontag/papers/SilSonFer_ECCV14.pdf'>Silberman, Sontag, and Fergus, Instance Segmentation of Indoor Scenes using a Coverage Loss</a>


