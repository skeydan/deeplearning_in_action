```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE)
#opts_chunk$set(out.width='750px', dpi=200)
```
<style>
.reveal table td{
    border: 0;
}
.reveal table {
    border: 0;
}
.reveal src {
    font-size: 0.6em;
}
.small-code pre code {
  font-size: 1em;
}
.midcenter {
    position: fixed;
    top: 50%;
    left: 50%;
}
.footer {
    position: fixed; 
    top: 90%;
    text-align:right; 
    width:90%;
    margin-top:-150px;
}
.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
</style>


Deep Learning in Action  
========================================================
autosize: true
width: 1440
incremental:true 

What brings you here?
========================================================

<table>
<tr>
<td><img src='self_driving_cars.png' width='400px' /></td><td><img src='translate.png' width='400px'/></td><td><img src='skincancer.png' width='400px'/></td>
</tr>
<tr>
<td><img src='go.png' width='400px' /></td><td><img src='stock.png' width='400px'/></td><td><img src='weather.png' width='400px'/></td>
</tr>
<tr><td class='src'>Sources: [1],[3],[4],[5],[6]</td></tr>
</table>

Sources
========================================================
incremental:false

[1] <a href='http://selfdrivingcars.mit.edu/'>MIT 6.S094: Deep Learning for Self-Driving Cars Lecture Slides</a>

[3] <a href='http://www.nature.com/nature/journal/v542/n7639/full/nature21056.html'>Esteva et al., Dermatologist-level classification of skin cancer with deep neural networks</a>

[4] <a href='https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol'>Wikipedia, AlphaGo versus Lee Sedol</a>

[5] <a href='www.sciedupress.com/journal/index.php/air/article/download/7720/5022'>Yoshihara et al., Leveraging temporal properties of news events for stock market prediction.</a>

[6] <a href='http://www.theweathercompany.com/newsroom/2017/01/23/seasonal-outlook-weather-company-predicts-unusually-mild-weather-early-february'</a> The Weather Company, Seasonal forecast</a>

[7] <a href= 'http://www.dddmag.com/news/2017/02/neural-network-learns-select-potential-anticancer-drugs'>Neural Network Learns to Select Potential Anticancer Drugs</a>

[8] <a href='https://research.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html'>Google Research Blog: AlphaGo: Mastering the ancient game of Go with Machine Learning</a>

[9] <a href='http://www.theweathercompany.com/DeepThunder'>The Weather Company Launches Deep Thunder - the World’s Most Advanced Hyper-Local Weather Forecasting Model for Businesses</a>

[10] <a href='https://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html'>Christos Stergiou and Dimitrios Siganos, Artificial neurons</a>

[11] <a href='https://uwaterloo.ca/data-science/sites/ca.data-science/files/uploads/files/lecture_1_0.pdf'>University of Waterloo Deep Learning Slides</a>

[12] <a href="http://www.deeplearningbook.org/"> Goodfellow et al., Deep Learning</a>





So what is a neural network?
========================================================

Biological neuron and artificial neuron

<table>
<tr>
<td><img src='neuron1.png' width='400px' /></td><td><img src='neuron2.png' width='400px'/></td>
</tr>
<tr><td class='src'>Source: [10]</td></tr>
</table>


Prototype of a neuron: the perceptron (Rosenblatt, 1958)
========================================================

<table>
<tr>
<td><img src='perceptron.png' width='800px' /></td>
</tr>
<tr><td class='src'>Source: [10]</td></tr>
</table>


Deep neural networks: introducing hidden layers
========================================================

<table>
<tr>
<td><img src='deep_nn.png' width='800px' /></td>
</tr>
<tr><td class='src'>Source: [10]</td></tr>
</table>


Deep neural networks as function composition
========================================================

&nbsp;

A deep representation is a composition of many functions

&nbsp;

$x \xrightarrow{w_1} h_1 \xrightarrow{w_2} h_2 \xrightarrow{w_3} ... \xrightarrow{w_n} h_n \xrightarrow{w_{n+1}} y$



Why go deep? A bit of background
========================================================

&nbsp;

Easy? Difficult?

- walk
- talk
- play chess
- solve matrix computations


Easy for us - difficult for computers
========================================================

&nbsp;

- controlled movement 
- speech recognition
- speech generation
- object recognition and object localization


Representation matters
========================================================

<table>
<tr>
<td><img src='coords.png' width='800px' /></td>
</tr>
<tr><td class='src'>Source: [12]</td></tr>
</table>



Just feed the network the right features?
========================================================

&nbsp;

What are the correct pixel values for a "bike" feature?

- race bike, mountain bike, e-bike?
- pixels in the shadow may be much darker
- what if bike is mostly obscured by rider standing in front?



Let the network pick the features
========================================================

... a layer at a time

<table>
<tr>
<td><img src='features.png' width='800px' /></td>
</tr>
<tr><td class='src'>Source: [12]</td></tr>
</table>



Backpropagation
========================================================
![optional caption text](backprop.png)

http://cs231n.github.io/optimization-2/

// If we anthropomorphize the circuit as wanting to output a higher value (which can help with intuition), then we can think of the circuit as “wanting” the output of the add gate to be lower (due to negative sign), and with a force of 4. To continue the recurrence and to chain the gradient, the add gate takes that gradient and multiplies it to all of the local gradients for its inputs (making the gradient on both x and y 1 * -4 = -4). 




So how does a network learn?
========================================================

&nbsp;

Just a sec - let's meet a real neural network first!

Play around in the browser:

- <a href='http://cs.stanford.edu/people/karpathy/convnetjs/index.html'>ConvNetJS</a>
- <a href='http://cs.stanford.edu/people/karpathy/convnetjs/index.html'>TensorFlow playground</a>











Why computer vision is hard
========================================================

![optional caption text](deformable_cat.png)


http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf


Tasks in computer vision
========================================================

![optional caption text](class_loc_dec_seg.png)

Source: CS231n (Stanford) lecture slides


Classification
========================================================

![optional caption text](classification.png)

A. Krizhevsky, I. Sutskever and G. Hinton, “ImageNet Classification with Deep Convolutional Neural Networks”, in Advances in Neural Information Processing Systems, 2012.

// output probability for each class

Localization
========================================================

![optional caption text](localization.png)

[1] Erhan D., Szegedy C., Toshev, A., and Anguelov, D., "Scalable Object Detection using Deep Neural Networks", The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 2147-2154.

// slide window over image
// or predict bounding box coordinates (regression)


Object Detection
========================================================

![optional caption text](detection.png)

http://image-net.org/challenges/LSVRC/2014/

// = classifying and localizing multiple objects in an image 
// aka image recognition
// binary mask regression for each object type combined with predicting the bounding box coordinates


Segmentation
========================================================

![optional caption text](segmentation.png)

https://arxiv.org/pdf/1411.4038.pdf

//the network needs to predict a class value for each input pixel.


Semantic vs. Instance Segmentation
========================================================

![optional caption text](segmentation.png)

https://arxiv.org/pdf/1411.4038.pdf

@inproceedings{SilbermanCoverage:ECCV14,
  author    = {Nathan Silberman, David Sontag and Rob Fergus},
  title     = {Instance Segmentation of Indoor Scenes using a Coverage Loss},
  booktitle = {ECCV},
  year      = {2014}
}


Convnet visualization
========================================================

https://arxiv.org/pdf/1311.2901.pdf


RNN
========================================================

colah

LSTM "conveyor belt"


Word2Vec
========================================================




Linking images and language: image caption generation
========================================================

![optional caption text](image_captions.png)
![optional caption text](image_captions2.png)

http://cs.stanford.edu/people/karpathy/cvpr2015.pdf


Linking video and sound: adding audio to silent film
========================================================

![optional caption text](sound_generation.png)

https://arxiv.org/pdf/1512.08512.pdf











Reinforcement learning: Welcome to the humans
========================================================

![optional caption text](typesofml.png) 

Source: deep driving

![optional caption text](dopamine.png) 

Reinforcement learning: Welcome to the humans
========================================================

> David Silver, Google Deep Mind: _Reinforcement Learning: AI = RL_

> Takeaway from Supervised Learning:
Neural networks are great at memorization and not (yet)
great at reasoning.
Hope for Reinforcement Learning:
Brute-force propagation of outcomes to knowledge about
states and actions. This is a kind of brute-force “reasoning”.
MIT DD


Delayed reward
========================================================

If I get a reward many many actions later, how do I find out what I'm getting the reward _for_?


Reinforcement learning
========================================================

![optional caption text](robot.png)

MIT



Deep reinforcement learning
========================================================

![optional caption text](dqn.png) 

http://www.nature.com/articles/nature16961.epdf?referrer_access_token=S7uXxNIroKd-0ITVLIW9mdRgN0jAjWel9jnR3ZoTv0OivKk3lXs6SxMz535byYwHnl5-dYSTNp9HCujoL8AwwR39NrI-N0UvQYqpO-G6W-1I6_OXAuVukQ08lbvopRKY2yVJlWWUJvj6gL5qyO8kI3FwsIuw4iSKC-s4RoTnZdVG8WevGFeuMdJ2Zl9cZF7yixAslaF4yKEx3rom3ZszmZBsyuq-9RAnx1XZac4keCI%3D&tracking_referrer=www.nature.com


Deep reinforcement learning
========================================================

![optional caption text](dqn2.png) 

http://icml.cc/2016/tutorials/AlphaGo-tutorial-slides.pdf




