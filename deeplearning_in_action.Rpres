```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE)
#opts_chunk$set(out.width='750px', dpi=200)
```
<style>
.reveal table td{
    border: 0;
}
.reveal table {
    border: 0;
}
.reveal src {
    font-size: 0.6em;
}
.small-code pre code {
  font-size: 1em;
}
.midcenter {
    position: fixed;
    top: 50%;
    left: 50%;
}
.footer {
    position: fixed; 
    top: 90%;
    text-align:right; 
    width:90%;
    margin-top:-150px;
}
.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
</style>


Deep Learning in Action  
========================================================
autosize: true
width: 1440
incremental:true 

Action!
========================================================

<table>
<tr>
<td><img src='self_driving_cars.png' width='300px' /></td><td><img src='translate.png' width='300px'/></td><td><img src='skincancer.png' width='300px'/></td>
</tr>
<tr>
<td><img src='go.png' width='300px' /></td><td><img src='stock.png' width='300px'/></td><td><img src='weather.png' width='300px'/></td>
</tr>
<tr><td class='src'>Sources: [1],[3],[4],[5],[6]</td></tr>
</table>


So what is a neural network?
========================================================

Biological neuron and artificial neuron

<table>
<tr>
<td><img src='neuron1.png' width='400px' /></td><td><img src='neuron2.png' width='400px'/></td>
</tr>
<tr><td class='src'>Source: [10]</td></tr>
</table>


Prototype of a neuron: the perceptron (Rosenblatt, 1958)
========================================================

<table>
<tr>
<td><img src='perceptron.png' width='800px' /></td>
</tr>
<tr><td class='src'>Source: [10]</td></tr>
</table>


Deep neural networks: introducing hidden layers
========================================================

<table>
<tr>
<td><img src='deep_nn.png' width='800px' /></td>
</tr>
<tr><td class='src'>Source: [10]</td></tr>
</table>


Deep neural networks as function composition
========================================================

&nbsp;

A deep representation is a composition of many functions

&nbsp;

$x \xrightarrow{w_1} h_1 \xrightarrow{w_2} h_2 \xrightarrow{w_3} ... \xrightarrow{w_n} h_n \xrightarrow{w_{n+1}} y$



Why go deep? A bit of background
========================================================

&nbsp;

Easy? Difficult?

- walk
- talk
- play chess
- solve matrix computations


Easy for us - difficult for computers
========================================================

&nbsp;

- controlled movement 
- speech recognition
- speech generation
- object recognition and object localization


Representation matters
========================================================

<table>
<tr>
<td><img src='coords.png' width='800px' /></td>
</tr>
<tr><td class='src'>Source: [12]</td></tr>
</table>



Just feed the network the right features?
========================================================

&nbsp;

What are the correct pixel values for a "bike" feature?

- race bike, mountain bike, e-bike?
- pixels in the shadow may be much darker
- what if bike is mostly obscured by rider standing in front?



Let the network pick the features
========================================================

... a layer at a time

<table>
<tr>
<td><img src='features.png' width='600px' /></td>
</tr>
<tr><td class='src'>Source: [12]</td></tr>
</table>



So how does a network learn?
========================================================

&nbsp;

Just a sec - let's meet a real neural network first!

Play around in the browser:

- <a href='http://cs.stanford.edu/people/karpathy/convnetjs/index.html'>ConvNetJS</a>
- <a href='http://cs.stanford.edu/people/karpathy/convnetjs/index.html'>TensorFlow playground</a>


So how DOES a neural network learn?
========================================================

&nbsp;

We need:

- a way to quantify our current (e.g., classification) error
- a way to reduce error on subsequent iterations
- a way to propagate our improvement logic from the output layer all back through the network!


Quantifying error: Loss functions
========================================================

&nbsp;

The _loss_ (or _cost_) function indicates the cost incurred from false prediction / misclassification

Probably the best-known loss function in machine learning is __mean squared error__: 

  $\frac{1}{n} \sum_n{(\hat{y} - y)^2}$
  
Most of the time, in deep learning we use __cross entropy__:

  $- \sum_j{t_j log(y_j)}$
  
This is the negative log probability of the right answer.
  



Learning from errors: Gradient Descent
========================================================


<table>
<tr>
<td><img src='convex.png' width='800px' /></td>
</tr>
<tr><td class='src'>Source: [12]</td></tr>
</table>


Propagate back errors ... well: Backpropagation!
========================================================
incremental:false

&nbsp;

- basically, just the chain rule: $\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}$
- chained over several layers:

<table>
<tr>
<td><img src='backprop2.png' width='600px' /></td>
</tr>
<tr><td class='src'>Source: [14]</td></tr>
</table>


 
Forward pass and backward pass: Intuition
========================================================
incremental:false

- imagine output of $f = (x + y) * z = -12$ "wants" to get bigger
- this could happen by $q$ getting smaller -> $q$ receives negative gradient $\frac{df}{dq} = -4$
- $q$ just passes on this gradient to $x$ and $y$, as  $\frac{dq}{dx} = 1$ and  $\frac{dq}{dy} = 1$
- alternatively, it could happen by $z$ getting bigger -> $z$ receives positive gradient $\frac{df}{dz} = 3$

<table>
<tr>
<td><img src='backprop.png' width='800px' /></td>
</tr>
<tr><td class='src'>Source: [13]</td></tr>
</table>



Applications by example
========================================================

&nbsp;

- CNNS (Convolutional Neural Networks) in computer vision
- Spee




Why computer vision is hard
========================================================

![optional caption text](deformable_cat.png)


http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf


Tasks in computer vision
========================================================

![optional caption text](class_loc_dec_seg.png)

Source: CS231n (Stanford) lecture slides


Classification
========================================================

![optional caption text](classification.png)

A. Krizhevsky, I. Sutskever and G. Hinton, “ImageNet Classification with Deep Convolutional Neural Networks”, in Advances in Neural Information Processing Systems, 2012.

// output probability for each class

Localization
========================================================

![optional caption text](localization.png)

[1] Erhan D., Szegedy C., Toshev, A., and Anguelov, D., "Scalable Object Detection using Deep Neural Networks", The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 2147-2154.

// slide window over image
// or predict bounding box coordinates (regression)


Object Detection
========================================================

![optional caption text](detection.png)

http://image-net.org/challenges/LSVRC/2014/

// = classifying and localizing multiple objects in an image 
// aka image recognition
// binary mask regression for each object type combined with predicting the bounding box coordinates


Segmentation
========================================================

![optional caption text](segmentation.png)

https://arxiv.org/pdf/1411.4038.pdf

//the network needs to predict a class value for each input pixel.


Semantic vs. Instance Segmentation
========================================================

![optional caption text](segmentation.png)

https://arxiv.org/pdf/1411.4038.pdf

@inproceedings{SilbermanCoverage:ECCV14,
  author    = {Nathan Silberman, David Sontag and Rob Fergus},
  title     = {Instance Segmentation of Indoor Scenes using a Coverage Loss},
  booktitle = {ECCV},
  year      = {2014}
}


Convnet visualization
========================================================

https://arxiv.org/pdf/1311.2901.pdf


RNN
========================================================

colah

LSTM "conveyor belt"


Word2Vec
========================================================




Linking images and language: image caption generation
========================================================

![optional caption text](image_captions.png)
![optional caption text](image_captions2.png)

http://cs.stanford.edu/people/karpathy/cvpr2015.pdf


Linking video and sound: adding audio to silent film
========================================================

![optional caption text](sound_generation.png)

https://arxiv.org/pdf/1512.08512.pdf











Reinforcement learning: Welcome to the humans
========================================================

![optional caption text](typesofml.png) 

Source: deep driving

![optional caption text](dopamine.png) 

Reinforcement learning: Welcome to the humans
========================================================

> David Silver, Google Deep Mind: _Reinforcement Learning: AI = RL_

> Takeaway from Supervised Learning:
Neural networks are great at memorization and not (yet)
great at reasoning.
Hope for Reinforcement Learning:
Brute-force propagation of outcomes to knowledge about
states and actions. This is a kind of brute-force “reasoning”.
MIT DD


Delayed reward
========================================================

If I get a reward many many actions later, how do I find out what I'm getting the reward _for_?


Reinforcement learning
========================================================

![optional caption text](robot.png)

MIT



Deep reinforcement learning
========================================================

![optional caption text](dqn.png) 

http://www.nature.com/articles/nature16961.epdf?referrer_access_token=S7uXxNIroKd-0ITVLIW9mdRgN0jAjWel9jnR3ZoTv0OivKk3lXs6SxMz535byYwHnl5-dYSTNp9HCujoL8AwwR39NrI-N0UvQYqpO-G6W-1I6_OXAuVukQ08lbvopRKY2yVJlWWUJvj6gL5qyO8kI3FwsIuw4iSKC-s4RoTnZdVG8WevGFeuMdJ2Zl9cZF7yixAslaF4yKEx3rom3ZszmZBsyuq-9RAnx1XZac4keCI%3D&tracking_referrer=www.nature.com


Deep reinforcement learning
========================================================

![optional caption text](dqn2.png) 

http://icml.cc/2016/tutorials/AlphaGo-tutorial-slides.pdf



Sources (1)
========================================================
incremental:false

&nbsp;

[1] <a href='http://selfdrivingcars.mit.edu/'>MIT 6.S094: Deep Learning for Self-Driving Cars Lecture Slides</a>

[3] <a href='http://www.nature.com/nature/journal/v542/n7639/full/nature21056.html'>Esteva et al., Dermatologist-level classification of skin cancer with deep neural networks</a>

[4] <a href='https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol'>Wikipedia, AlphaGo versus Lee Sedol</a>

[5] <a href='www.sciedupress.com/journal/index.php/air/article/download/7720/5022'>Yoshihara et al., Leveraging temporal properties of news events for stock market prediction.</a>

[6] <a href='http://www.theweathercompany.com/newsroom/2017/01/23/seasonal-outlook-weather-company-predicts-unusually-mild-weather-early-february'</a> The Weather Company, Seasonal forecast</a>

[7] <a href= 'http://www.dddmag.com/news/2017/02/neural-network-learns-select-potential-anticancer-drugs'>Neural Network Learns to Select Potential Anticancer Drugs</a>

[8] <a href='https://research.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html'>Google Research Blog: AlphaGo: Mastering the ancient game of Go with Machine Learning</a>

[9] <a href='http://www.theweathercompany.com/DeepThunder'>The Weather Company Launches Deep Thunder - the World’s Most Advanced Hyper-Local Weather Forecasting Model for Businesses</a>


Sources (2)
========================================================
incremental:false

&nbsp;

[10] <a href='https://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html'>Christos Stergiou and Dimitrios Siganos, Artificial neurons</a>

[11] <a href='https://uwaterloo.ca/data-science/sites/ca.data-science/files/uploads/files/lecture_1_0.pdf'>University of Waterloo Deep Learning Slides</a>

[12] <a href="http://www.deeplearningbook.org/"> Goodfellow et al., Deep Learning</a>

[13] <a href='http://cs231n.github.io/optimization-2/'>CS231n: Convolutional Neural Networks for Visual Recognition, Stanford</a>

[14] <a href='https://colah.github.io/posts/2015-08-Backprop/'>Chris Olah, Calculus on Computational Graphs: Backpropagation</a>

